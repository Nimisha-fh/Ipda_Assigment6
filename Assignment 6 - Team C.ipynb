{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "web scrapeing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping details for PZN: 18201503\n",
      "Scraping details for PZN: 18215149\n",
      "Scraping details for PZN: 12550409\n",
      "Scraping details for PZN: 17674903\n",
      "Scraping details for PZN: 2830585\n",
      "Scraping details for PZN: 18110232\n",
      "Scraping details for PZN: 10787337\n",
      "Scraping details for PZN: 9651609\n",
      "Scraping details for PZN: 16599832\n",
      "Scraping details for PZN: 9461168\n",
      "Scraping details for PZN: 17553447\n",
      "Scraping details for PZN: 17268534\n",
      "Scraping details for PZN: 18110226\n",
      "Scraping details for PZN: 16760115\n",
      "Scraping details for PZN: 16703229\n",
      "Scraping details for PZN: 7641311\n",
      "Scraping details for PZN: 17313387\n",
      "Scraping details for PZN: 679612\n",
      "Scraping details for PZN: 16330656\n",
      "Scraping details for PZN: 16153036\n",
      "Scraping details for PZN: 15201158\n",
      "Scraping details for PZN: 2830579\n",
      "Scraping details for PZN: 1852188\n",
      "Scraping details for PZN: 1448435\n",
      "Scraping details for PZN: 2483617\n",
      "Scraping details for PZN: 16762410\n",
      "Scraping details for PZN: 14163898\n",
      "Scraping details for PZN: 4979274\n",
      "Scraping details for PZN: 16760121\n",
      "Scraping details for PZN: 13827988\n",
      "Scraping details for PZN: 11077448\n",
      "Scraping details for PZN: 14163912\n",
      "Scraping details for PZN: 14017493\n",
      "Scraping details for PZN: 16739718\n",
      "Scraping details for PZN: 9098348\n",
      "Scraping details for PZN: 1805349\n",
      "Scraping details for PZN: 1447157\n",
      "Scraping details for PZN: 562560\n",
      "Scraping details for PZN: 18259098\n",
      "Scraping details for PZN: 9635303\n",
      "Scraping details for PZN: 9442171\n",
      "Scraping details for PZN: 999854\n",
      "Scraping details for PZN: 6569847\n",
      "Scraping details for PZN: 10203603\n",
      "Scraping details for PZN: 18671350\n",
      "Scraping details for PZN: 18232975\n",
      "Scraping details for PZN: 17154204\n",
      "Scraping details for PZN: 11553995\n",
      "Scraping details for PZN: 18440834\n",
      "Scraping details for PZN: 19067596\n",
      "Scraping details for PZN: 14020911\n",
      "Scraping details for PZN: 7728360\n",
      "Scraping details for PZN: 3273313\n",
      "Scraping details for PZN: 16397241\n",
      "Scraping details for PZN: 12421563\n",
      "Scraping details for PZN: 6873114\n",
      "Scraping details for PZN: 17553565\n",
      "Scraping details for PZN: 9432238\n",
      "Scraping details for PZN: 14163929\n",
      "Scraping details for PZN: 17268528\n",
      "Scraping details for PZN: 3147258\n",
      "Scraping details for PZN: 17268540\n",
      "Scraping details for PZN: 18021942\n",
      "Scraping details for PZN: 15193536\n",
      "Scraping details for PZN: 3929274\n",
      "Scraping details for PZN: 10264763\n",
      "Scraping details for PZN: 10268672\n",
      "Scraping details for PZN: 16015587\n",
      "Scraping details for PZN: 16538227\n",
      "Scraping details for PZN: 16349822\n",
      "Scraping details for PZN: 10089596\n",
      "Scraping details for PZN: 18489042\n",
      "Scraping details for PZN: 6571703\n",
      "Scraping details for PZN: 16760090\n",
      "Scraping details for PZN: 4830483\n",
      "Scraping details for PZN: 11678047\n",
      "Scraping details for PZN: 16507534\n",
      "Scraping details for PZN: 10333719\n",
      "Scraping details for PZN: 5874560\n",
      "Scraping details for PZN: 16742749\n",
      "Scraping details for PZN: 16759224\n",
      "Scraping details for PZN: 14372283\n",
      "Scraping details for PZN: 1580241\n",
      "Scraping details for PZN: 499181\n",
      "Scraping details for PZN: 11024417\n",
      "Scraping details for PZN: 4956037\n",
      "Scraping details for PZN: 13170548\n",
      "Scraping details for PZN: 14331309\n",
      "Scraping details for PZN: 12531748\n",
      "Scraping details for PZN: 16760150\n",
      "Scraping details for PZN: 1875143\n",
      "Scraping details for PZN: 16762462\n",
      "Scraping details for PZN: 11663531\n",
      "Scraping details for PZN: 14190180\n",
      "Scraping details for PZN: 12551047\n",
      "Scraping details for PZN: 3227112\n",
      "Scraping details for PZN: 11287074\n",
      "Scraping details for PZN: 18052351\n",
      "Scraping details for PZN: 13833434\n",
      "Scraping details for PZN: 13831180\n",
      "Scraping details for PZN: 4775123\n",
      "Scraping details for PZN: 306437\n",
      "Scraping details for PZN: 250777\n",
      "Scraping details for PZN: 15395234\n",
      "Scraping details for PZN: 3126523\n",
      "Scraping details for PZN: 361790\n",
      "Scraping details for PZN: 9702011\n",
      "Scraping details for PZN: 7555072\n",
      "Scraping details for PZN: 7243036\n",
      "Scraping details for PZN: 4796869\n",
      "Scraping details for PZN: 4074946\n",
      "Scraping details for PZN: 1578681\n",
      "Scraping details for PZN: 660469\n",
      "Scraping details for PZN: 16739724\n",
      "Scraping details for PZN: 16730640\n",
      "Scraping details for PZN: 10798358\n",
      "Scraping details for PZN: 9042974\n",
      "Scraping details for PZN: 4274616\n",
      "Scraping details for PZN: 2882760\n",
      "Scraping details for PZN: 1689848\n",
      "Scraping details for PZN: 2477433\n",
      "Scraping details for PZN: 2330807\n",
      "Scraping details for PZN: 15293775\n",
      "Scraping details for PZN: 10065578\n",
      "Scraping details for PZN: 9932544\n",
      "Scraping details for PZN: 329289\n",
      "Scraping details for PZN: 743422\n",
      "Scraping details for PZN: 16124135\n",
      "Scraping details for PZN: 3643448\n",
      "Scraping details for PZN: 2801069\n",
      "Scraping details for PZN: 3040980\n",
      "Scraping details for PZN: 3815725\n",
      "Scraping details for PZN: 12574692\n",
      "Scraping details for PZN: 927263\n",
      "Scraping details for PZN: 19155567\n",
      "Scraping details for PZN: 10713497\n",
      "Scraping details for PZN: 4522540\n",
      "Scraping details for PZN: 14017487\n",
      "Scraping details for PZN: 15401259\n",
      "Scraping details for PZN: 13947008\n",
      "Scraping details for PZN: 9924295\n",
      "Scraping details for PZN: 10123637\n",
      "Scraping details for PZN: 11363467\n",
      "Scraping details for PZN: 91089\n",
      "Scraping details for PZN: 16744895\n",
      "Scraping details for PZN: 3709816\n",
      "Scraping details for PZN: 16932881\n",
      "Scraping details for PZN: 109470\n",
      "Scraping details for PZN: 17862148\n",
      "Scraping details for PZN: 16154478\n",
      "Scraping completed and saved to CSV.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Function to scrape product info\n",
    "def scrape_pzn_details(pzn):\n",
    "    try:\n",
    "        url = f\"https://www.shop-apotheke.com/arzneimittel/{pzn}/\"  \n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            return {\"pzn\": pzn, \"error\": f\"Failed to retrieve data, status code: {response.status_code}\"}\n",
    "\n",
    "        # Parse the HTML\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find(\"title\").text.strip() if soup.find(\"title\") else \"N/A\"\n",
    "\n",
    "        # Extract price (from meta description or similar tag)\n",
    "        meta_desc = soup.find(\"meta\", attrs={\"name\": \"description\"})\n",
    "        description = meta_desc[\"content\"] if meta_desc else \"N/A\"\n",
    "        \n",
    "        # Extract price from description (or any available price tag)\n",
    "        price = \"N/A\"\n",
    "        if \"€\" in description:\n",
    "            price = description.split(\"€\")[1].split(\" \")[0].strip()\n",
    "\n",
    "        # Extract canonical URL (or use a pattern to get the product URL)\n",
    "        canonical_url = soup.find(\"link\", rel=\"canonical\")\n",
    "        product_url = canonical_url[\"href\"] if canonical_url else \"N/A\"\n",
    "\n",
    "        # Extract category/brand from description (if available)\n",
    "        category = \"N/A\"\n",
    "        brand = \"N/A\"\n",
    "        if description:\n",
    "            # Example for brand/category identification logic, modify as necessary\n",
    "            if \"Eucerin\" in description:\n",
    "                brand = \"Eucerin\"\n",
    "            if \"Sunscreen\" in description:\n",
    "                category = \"Sunscreen\"\n",
    "\n",
    "        return {\n",
    "            \"pzn\": pzn,\n",
    "            \"title\": title,\n",
    "            \"price\": price,\n",
    "            \"description\": description\n",
    "           \n",
    "            \n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"pzn\": pzn, \"error\": str(e)}\n",
    "\n",
    "\n",
    "pzn_data = pd.read_csv(r\"C:\\Users\\NIMISHA\\Downloads\\20241114_export_pharmacy_receiptdata_products.csv\", header=None)\n",
    "pzn_data.columns = ['pzn', 'title']\n",
    "pzn_counts = pzn_data['pzn'].value_counts()\n",
    "selected_pzns = pzn_counts.head(200).tail(150) \n",
    "# Scraping data for selected PZNs\n",
    "results = []\n",
    "for pzn in selected_pzns.index:\n",
    "    print(f\"Scraping details for PZN: {pzn}\")\n",
    "    result = scrape_pzn_details(pzn)\n",
    "    results.append(result)\n",
    "    time.sleep(2)  # Delay to avoid rate limiting\n",
    "\n",
    "# Create a DataFrame and save the results to CSV\n",
    "scraped_df = pd.DataFrame(results)\n",
    "scraped_df.to_csv(r\"C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\scraped_pzn_data.csv\", index=False)\n",
    "\n",
    "print(\"Scraping completed and saved to CSV.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define their category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Sun & Skin Care products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\sun_&_skin_care.csv\n",
      "Saved Pain & Wound Care products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\pain_&_wound_care.csv\n",
      "Saved Cold, Allergy & Respiratory products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\cold,_allergy_&_respiratory.csv\n",
      "Saved Oral & Dental Care products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\oral_&_dental_care.csv\n",
      "Saved Baby & Kids products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\baby_&_kids.csv\n",
      "Saved Vitamins & Supplements products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\vitamins_&_supplements.csv\n",
      "Saved Insect Repellents & Protection products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\insect_repellents_&_protection.csv\n",
      "Saved Cold Sore & Herpes Treatment products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\cold_sore_&_herpes_treatment.csv\n",
      "Saved Disinfection & Antiseptics products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\disinfection_&_antiseptics.csv\n",
      "Saved Serum & Skincare products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\serum_&_skincare.csv\n",
      "Saved Tablets & Capsules products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\tablets_&_capsules.csv\n",
      "Saved Smoking Cessation products to C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\\smoking_cessation.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the scraped data\n",
    "file_path = r\"C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\scraped_pzn_data.csv\"\n",
    "scraped_df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Remove entries where both title and description are missing\n",
    "scraped_df.dropna(subset=[\"title\", \"description\"], how=\"all\", inplace=True)\n",
    "\n",
    "# Step 2: Ensure 'category' column exists\n",
    "if 'category' not in scraped_df.columns:\n",
    "    scraped_df['category'] = 'Uncategorized'  # Default to 'Uncategorized' if the column does not exist\n",
    "\n",
    "# Step 3: Define broader categories with relevant keywords\n",
    "category_groups = {\n",
    "    \"Sun & Skin Care\": [\"sun\", \"sunscreen\", \"uv\", \"spf\", \"photoaging\", \"skin\", \"cream\", \"anti-pigment\", \"cerave\", \"eucerin\", \"hydrocortison\", \"eczema\", \"sensitive\"],\n",
    "    \"Pain & Wound Care\": [\"TIGER BALM\", \"wund\", \"aspirin\", \"fußpilzerkrankungen\", \"pain\", \"fingerverband\", \"schmerz\", \"voltaren\", \"gel\", \"diclofenac\", \"ibu-lysin\", \"ibuprofen\", \"wound\", \"hansaplast\", \"plaster\", \"spray\", \"aqua protect\", \"sensitive xxl\"],\n",
    "    \"Cold, Allergy & Respiratory\": [\"cold\", \"allergy\", \"nasal\", \"mometahexal\", \"contramutan\", \"nasenspray\", \"hustensaft\", \"inhaler\", \"spray\"],\n",
    "    \"Oral & Dental Care\": [\"oral\", \"tepe\", \"interdental\", \"mouth\", \"dental\", \"toothpaste\"],\n",
    "    \"Baby & Kids\": [\"baby\", \"junior\", \"kids\"],\n",
    "    \"Vitamins & Supplements\": [\"vitamin\", \"folio\", \"dekristol\", \"b5\", \"supplement\", \"magnesium\", \"imodium\", \"multivitamin\", \"zinc\", \"iron\"],\n",
    "    \"Insect Repellents & Protection\": [\"insect\", \"repellent\", \"anti brumm\", \"mosquito\"],\n",
    "    \"Cold Sore & Herpes Treatment\": [\"herpes\", \"pencivir\", \"compeed\", \"acyclovir\"],\n",
    "    \"Disinfection & Antiseptics\": [\"disinfect\", \"pvp-jod\", \"octenisept\", \"antiseptic\", \"iodine\"],\n",
    "    \"Serum & Skincare\": [\"serum\", \"hyaluronic\", \"retinol\", \"niacinamide\"],\n",
    "    \"Beauty & Cosmetics\": [\"beauty\", \"cosmetic\", \"foundation\", \"lipstick\", \"makeup\", \"mascara\", \"blush\"],\n",
    "    \"Creams & Ointments\": [\"cream\", \"lotion\", \"ointment\", \"moisturizer\", \"gel\"],\n",
    "    \"Tablets & Capsules\": [\"tabs\", \"capsule\", \"pills\", \"dragees\"],\n",
    "    \"Smoking Cessation\": [\"nicorette\", \"tx pflaster\", \"smoking cessation\", \"nicotine patch\"]\n",
    "}\n",
    "\n",
    "# Step 4: Compile category-specific regex patterns for faster matching\n",
    "category_patterns = {}\n",
    "for category, keywords in category_groups.items():\n",
    "    # Create a regex pattern that matches any of the keywords in the category\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, keywords)) + r')\\b'\n",
    "    category_patterns[category] = re.compile(pattern, re.IGNORECASE)\n",
    "\n",
    "# Step 5: Assign category based on keywords using the compiled regex patterns\n",
    "def assign_category(row):\n",
    "    title = str(row[\"title\"]).lower() if pd.notna(row[\"title\"]) else \"\"\n",
    "    description = str(row[\"description\"]).lower() if pd.notna(row[\"description\"]) else \"\"\n",
    "    combined_text = title + \" \" + description  # Combine both title and description for better matching\n",
    "\n",
    "    for category, pattern in category_patterns.items():\n",
    "        if pattern.search(combined_text):  # Match against the compiled pattern\n",
    "            return category\n",
    "    return \"Uncategorized\"\n",
    "\n",
    "# Apply the categorization function\n",
    "scraped_df[\"category\"] = scraped_df.apply(assign_category, axis=1)\n",
    "\n",
    "# Step 6: Remove unwanted columns ('brand' and 'error') if they exist\n",
    "columns_to_remove = ['brand', 'error']\n",
    "scraped_df.drop(columns=[col for col in columns_to_remove if col in scraped_df.columns], inplace=True)\n",
    "\n",
    "# Step 7: Save categorized data into group-wise CSVs\n",
    "output_folder = r\"C:\\Users\\NIMISHA\\Desktop\\Assigment6 new\\categorized_data\"\n",
    "os.makedirs(output_folder, exist_ok=True)  # Ensure the output folder exists\n",
    "\n",
    "# Save products for each category in separate CSV files\n",
    "for category in category_groups.keys():\n",
    "    category_df = scraped_df[scraped_df[\"category\"] == category]\n",
    "    if not category_df.empty:\n",
    "        # Secure file naming by removing any special characters\n",
    "        category_file = os.path.join(output_folder, f\"{category.replace(' ', '_').lower()}.csv\")\n",
    "        category_df.to_csv(category_file, index=False)\n",
    "        print(f\"Saved {category} products to {category_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All PZNs are valid.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_pzn_check_digit(pzn_base):\n",
    "    \"\"\"\n",
    "    Calculate the check digit for a PZN using the Modulo 11 algorithm.\n",
    "    Args:\n",
    "        pzn_base (str): The base part of the PZN (7 digits).\n",
    "    Returns:\n",
    "        int: The check digit, or -1 if the calculation results in 10 (invalid PZN).\n",
    "    \"\"\"\n",
    "    if not pzn_base.isdigit() or len(pzn_base) != 7:\n",
    "        return -1  # Invalid input\n",
    "    \n",
    "    weights = range(1, 8)  # Weights from 1 to 7\n",
    "    weighted_sum = sum(int(digit) * weight for digit, weight in zip(pzn_base, weights))\n",
    "    check_digit = weighted_sum % 11\n",
    "    \n",
    "    return check_digit if check_digit != 10 else -1  # Return -1 for invalid PZNs\n",
    "\n",
    "\n",
    "def validate_pzn(pzn):\n",
    "    \"\"\"\n",
    "    Validate a complete PZN by checking its check digit.\n",
    "    Args:\n",
    "        pzn (str): The full PZN (8 digits, including the check digit).\n",
    "    Returns:\n",
    "        bool: True if the PZN is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    if not pzn.isdigit() or len(pzn) != 8:\n",
    "        return False  # Invalid format or length\n",
    "    \n",
    "    pzn_base, actual_check_digit = pzn[:-1], int(pzn[-1])\n",
    "    expected_check_digit = calculate_pzn_check_digit(pzn_base)\n",
    "    \n",
    "    return actual_check_digit == expected_check_digit\n",
    "\n",
    "\n",
    "# Updated file path\n",
    "csv_file_path = r\"C:\\Users\\NIMISHA\\Downloads\\20241114_export_pharmacy_receiptdata_products.csv\"\n",
    "\n",
    "# Load the dataset (assuming no header)\n",
    "cleaned_data = pd.read_csv(csv_file_path, header=None, dtype=str)\n",
    "\n",
    "# Assign column names manually (assuming first column is PZN, second is title/product name)\n",
    "cleaned_data.columns = ['pzn', 'title']\n",
    "\n",
    "# Ensure 'pzn' is a string, remove spaces, and pad with leading zeros if necessary\n",
    "cleaned_data['pzn'] = cleaned_data['pzn'].astype(str).str.strip().str.zfill(8)  # Ensures all PZNs are 8 digits\n",
    "\n",
    "# Apply PZN validation\n",
    "cleaned_data['is_valid_pzn'] = cleaned_data['pzn'].apply(validate_pzn)\n",
    "\n",
    "# Extract invalid PZNs\n",
    "invalid_pzns = cleaned_data[~cleaned_data['is_valid_pzn']]\n",
    "\n",
    "if not invalid_pzns.empty:\n",
    "    print(f\"Found {len(invalid_pzns)} invalid PZNs:\")\n",
    "    print(invalid_pzns[['pzn', 'title']])\n",
    "\n",
    "    # Save invalid PZNs to a CSV for review\n",
    "    invalid_pzns.to_csv(r\"C:\\Users\\NIMISHA\\Downloads\\invalid_pzns.csv\", index=False)\n",
    "    print(\"Invalid PZNs saved to 'invalid_pzns.csv'.\")\n",
    "else:\n",
    "    print(\"All PZNs are valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 invalid PZNs:\n",
      "Empty DataFrame\n",
      "Columns: [pzn, title]\n",
      "Index: []\n",
      "Invalid PZNs saved to 'invalid_pzns.csv'.\n",
      "Cleaned and validated PZN data saved to 'cleaned_validated_pzn_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def calculate_pzn_check_digit(pzn_base):\n",
    "    \"\"\"\n",
    "    Calculate the check digit for a PZN using the Modulo 11 algorithm.\n",
    "    Args:\n",
    "        pzn_base (str): The base part of the PZN (7 digits).\n",
    "    Returns:\n",
    "        int: The check digit, or -1 if the calculation results in 10 (invalid PZN).\n",
    "    \"\"\"\n",
    "    if not pzn_base.isdigit() or len(pzn_base) != 7:\n",
    "        raise ValueError(\"PZN base must be a 7-digit numeric string.\")\n",
    "    \n",
    "    weights = range(1, 8)  # Weights from 1 to 7\n",
    "    weighted_sum = sum(int(digit) * weight for digit, weight in zip(pzn_base, weights))\n",
    "    check_digit = weighted_sum % 11\n",
    "\n",
    "    return check_digit if check_digit != 10 else -1  # Return -1 for invalid PZNs\n",
    "\n",
    "\n",
    "def validate_pzn(pzn):\n",
    "    \"\"\"\n",
    "    Validate a complete PZN by checking its check digit.\n",
    "    Args:\n",
    "        pzn (str): The full PZN (8 digits, including the check digit).\n",
    "    Returns:\n",
    "        bool: True if the PZN is valid, False otherwise.\n",
    "    \"\"\"\n",
    "    pzn = str(pzn).strip().zfill(8)  # Ensure PZN is a string and pad missing zeros\n",
    "    \n",
    "    if not pzn.isdigit() or len(pzn) != 8:\n",
    "        return False  # Invalid format or length\n",
    "    \n",
    "    pzn_base, actual_check_digit = pzn[:-1], int(pzn[-1])\n",
    "    expected_check_digit = calculate_pzn_check_digit(pzn_base)\n",
    "    \n",
    "    return actual_check_digit == expected_check_digit\n",
    "\n",
    "\n",
    "#  Load and Process the Data\n",
    "cleaned_file_path = r\"C:\\Users\\NIMISHA\\Downloads\\20241114_export_pharmacy_receiptdata_products.csv\"\n",
    "\n",
    "# Load CSV without headers \n",
    "cleaned_data = pd.read_csv(cleaned_file_path, dtype=str, header=None)\n",
    "\n",
    "# Assign column names manually if missing\n",
    "cleaned_data.columns = [\"pzn\", \"title\"]  # Update based on actual data\n",
    "\n",
    "# Ensure 'pzn' is always a string and preserve leading zeros\n",
    "cleaned_data[\"pzn\"] = cleaned_data[\"pzn\"].astype(str).str.strip().str.zfill(8)\n",
    "\n",
    "#  Apply PZN Validation**\n",
    "cleaned_data['is_valid_pzn'] = cleaned_data['pzn'].apply(validate_pzn)\n",
    "\n",
    "# Step 3: Extract and Save Invalid PZNs\n",
    "invalid_pzns = cleaned_data[~cleaned_data['is_valid_pzn']]\n",
    "print(f\"Found {len(invalid_pzns)} invalid PZNs:\")\n",
    "print(invalid_pzns[['pzn', 'title']])\n",
    "\n",
    "# Save invalid PZNs to a CSV for review\n",
    "invalid_pzns.to_csv(\"invalid_pzns.csv\", index=False)\n",
    "print(\"Invalid PZNs saved to 'invalid_pzns.csv'.\")\n",
    "\n",
    "#  Save the cleaned & validated data\n",
    "cleaned_data.to_csv(\"cleaned_validated_pzn_data.csv\", index=False)\n",
    "print(\"Cleaned and validated PZN data saved to 'cleaned_validated_pzn_data.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IPDA_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
